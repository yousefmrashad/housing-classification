{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Housing Prices Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:59:02.387356300Z",
     "start_time": "2023-05-12T14:59:02.005697500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the dataset\n",
    "The dataset file 'Ames_Housing_Sales.csv' is read using `pandas.csv_reader`, and stored in a `pandas.DataFrame`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:59:02.676612700Z",
     "start_time": "2023-05-12T14:59:02.021077800Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Ames_Housing_Sales.csv', na_values='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:59:02.680612300Z",
     "start_time": "2023-05-12T14:59:02.045165900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   1stFlrSF  2ndFlrSF  3SsnPorch Alley  BedroomAbvGr BldgType BsmtCond   \n0     856.0     854.0        0.0   NaN             3     1Fam       TA  \\\n1    1262.0       0.0        0.0   NaN             3     1Fam       TA   \n2     920.0     866.0        0.0   NaN             3     1Fam       TA   \n3     961.0     756.0        0.0   NaN             3     1Fam       Gd   \n4    1145.0    1053.0        0.0   NaN             4     1Fam       TA   \n\n  BsmtExposure  BsmtFinSF1  BsmtFinSF2  ... ScreenPorch Street  TotRmsAbvGrd   \n0           No       706.0         0.0  ...         0.0   Pave             8  \\\n1           Gd       978.0         0.0  ...         0.0   Pave             6   \n2           Mn       486.0         0.0  ...         0.0   Pave             6   \n3           No       216.0         0.0  ...         0.0   Pave             7   \n4           Av       655.0         0.0  ...         0.0   Pave             9   \n\n   TotalBsmtSF Utilities  WoodDeckSF YearBuilt YearRemodAdd YrSold SalePrice  \n0        856.0    AllPub         0.0      2003         2003   2008  208500.0  \n1       1262.0    AllPub       298.0      1976         1976   2007  181500.0  \n2        920.0    AllPub         0.0      2001         2002   2008  223500.0  \n3        756.0    AllPub         0.0      1915         1970   2006  140000.0  \n4       1145.0    AllPub       192.0      2000         2000   2008  250000.0  \n\n[5 rows x 80 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>3SsnPorch</th>\n      <th>Alley</th>\n      <th>BedroomAbvGr</th>\n      <th>BldgType</th>\n      <th>BsmtCond</th>\n      <th>BsmtExposure</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>ScreenPorch</th>\n      <th>Street</th>\n      <th>TotRmsAbvGrd</th>\n      <th>TotalBsmtSF</th>\n      <th>Utilities</th>\n      <th>WoodDeckSF</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>YrSold</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>856.0</td>\n      <td>854.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>706.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>Pave</td>\n      <td>8</td>\n      <td>856.0</td>\n      <td>AllPub</td>\n      <td>0.0</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>2008</td>\n      <td>208500.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1262.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>Gd</td>\n      <td>978.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>Pave</td>\n      <td>6</td>\n      <td>1262.0</td>\n      <td>AllPub</td>\n      <td>298.0</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>2007</td>\n      <td>181500.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>920.0</td>\n      <td>866.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>Mn</td>\n      <td>486.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>Pave</td>\n      <td>6</td>\n      <td>920.0</td>\n      <td>AllPub</td>\n      <td>0.0</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>2008</td>\n      <td>223500.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>961.0</td>\n      <td>756.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>1Fam</td>\n      <td>Gd</td>\n      <td>No</td>\n      <td>216.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>Pave</td>\n      <td>7</td>\n      <td>756.0</td>\n      <td>AllPub</td>\n      <td>0.0</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>2006</td>\n      <td>140000.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1145.0</td>\n      <td>1053.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>Av</td>\n      <td>655.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>Pave</td>\n      <td>9</td>\n      <td>1145.0</td>\n      <td>AllPub</td>\n      <td>192.0</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>2008</td>\n      <td>250000.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 80 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the dataset in table format\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`DataFrame.info()` is used to provide information about each data feature (`column`), namely, the name, the number of non-null entries, and the datatype(`Dtype`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:59:02.709338300Z",
     "start_time": "2023-05-12T14:59:02.062045700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1379 entries, 0 to 1378\n",
      "Data columns (total 80 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   1stFlrSF       1379 non-null   float64\n",
      " 1   2ndFlrSF       1379 non-null   float64\n",
      " 2   3SsnPorch      1379 non-null   float64\n",
      " 3   Alley          82 non-null     object \n",
      " 4   BedroomAbvGr   1379 non-null   int64  \n",
      " 5   BldgType       1379 non-null   object \n",
      " 6   BsmtCond       953 non-null    object \n",
      " 7   BsmtExposure   953 non-null    object \n",
      " 8   BsmtFinSF1     1379 non-null   float64\n",
      " 9   BsmtFinSF2     1379 non-null   float64\n",
      " 10  BsmtFinType1   953 non-null    object \n",
      " 11  BsmtFinType2   952 non-null    object \n",
      " 12  BsmtFullBath   1379 non-null   int64  \n",
      " 13  BsmtHalfBath   1379 non-null   int64  \n",
      " 14  BsmtQual       953 non-null    object \n",
      " 15  BsmtUnfSF      1379 non-null   float64\n",
      " 16  CentralAir     1379 non-null   object \n",
      " 17  Condition1     1379 non-null   object \n",
      " 18  Condition2     1379 non-null   object \n",
      " 19  Electrical     1379 non-null   object \n",
      " 20  EnclosedPorch  1379 non-null   float64\n",
      " 21  ExterCond      1379 non-null   object \n",
      " 22  ExterQual      1379 non-null   object \n",
      " 23  Exterior1st    1379 non-null   object \n",
      " 24  Exterior2nd    1379 non-null   object \n",
      " 25  Fence          265 non-null    object \n",
      " 26  FireplaceQu    761 non-null    object \n",
      " 27  Fireplaces     1379 non-null   int64  \n",
      " 28  Foundation     1379 non-null   object \n",
      " 29  FullBath       1379 non-null   int64  \n",
      " 30  Functional     1379 non-null   object \n",
      " 31  GarageArea     1379 non-null   float64\n",
      " 32  GarageCars     1379 non-null   int64  \n",
      " 33  GarageCond     1379 non-null   object \n",
      " 34  GarageFinish   1379 non-null   object \n",
      " 35  GarageQual     1379 non-null   object \n",
      " 36  GarageType     1379 non-null   object \n",
      " 37  GarageYrBlt    1379 non-null   float64\n",
      " 38  GrLivArea      1379 non-null   float64\n",
      " 39  HalfBath       1379 non-null   int64  \n",
      " 40  Heating        1379 non-null   object \n",
      " 41  HeatingQC      1379 non-null   object \n",
      " 42  HouseStyle     1379 non-null   object \n",
      " 43  KitchenAbvGr   1379 non-null   int64  \n",
      " 44  KitchenQual    1379 non-null   object \n",
      " 45  LandContour    1379 non-null   object \n",
      " 46  LandSlope      1379 non-null   object \n",
      " 47  LotArea        1379 non-null   float64\n",
      " 48  LotConfig      1379 non-null   object \n",
      " 49  LotFrontage    1379 non-null   float64\n",
      " 50  LotShape       1379 non-null   object \n",
      " 51  LowQualFinSF   1379 non-null   float64\n",
      " 52  MSSubClass     1379 non-null   int64  \n",
      " 53  MSZoning       1379 non-null   object \n",
      " 54  MasVnrArea     1379 non-null   float64\n",
      " 55  MasVnrType     582 non-null    object \n",
      " 56  MiscFeature    51 non-null     object \n",
      " 57  MiscVal        1379 non-null   float64\n",
      " 58  MoSold         1379 non-null   int64  \n",
      " 59  Neighborhood   1379 non-null   object \n",
      " 60  OpenPorchSF    1379 non-null   float64\n",
      " 61  OverallCond    1379 non-null   int64  \n",
      " 62  OverallQual    1379 non-null   int64  \n",
      " 63  PavedDrive     1379 non-null   object \n",
      " 64  PoolArea       1379 non-null   float64\n",
      " 65  PoolQC         7 non-null      object \n",
      " 66  RoofMatl       1379 non-null   object \n",
      " 67  RoofStyle      1379 non-null   object \n",
      " 68  SaleCondition  1379 non-null   object \n",
      " 69  SaleType       1379 non-null   object \n",
      " 70  ScreenPorch    1379 non-null   float64\n",
      " 71  Street         1379 non-null   object \n",
      " 72  TotRmsAbvGrd   1379 non-null   int64  \n",
      " 73  TotalBsmtSF    1379 non-null   float64\n",
      " 74  Utilities      1379 non-null   object \n",
      " 75  WoodDeckSF     1379 non-null   float64\n",
      " 76  YearBuilt      1379 non-null   int64  \n",
      " 77  YearRemodAdd   1379 non-null   int64  \n",
      " 78  YrSold         1379 non-null   int64  \n",
      " 79  SalePrice      1379 non-null   float64\n",
      "dtypes: float64(21), int64(16), object(43)\n",
      "memory usage: 862.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#displaying information for each feature (column)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Null Values\n",
    "the `DataFrame.isna()` function of the DataFrame is used to return the subset of the data containing `null` values and storing the `sum()` into `na_vals`.\n",
    "The subset of columns containing null values is displayed along with the number of null values for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:59:02.712354200Z",
     "start_time": "2023-05-12T14:59:02.076653600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == 11 Columns ==\n",
      "Alley           1297\n",
      "BsmtCond         426\n",
      "BsmtExposure     426\n",
      "BsmtFinType1     426\n",
      "BsmtFinType2     427\n",
      "BsmtQual         426\n",
      "Fence           1114\n",
      "FireplaceQu      618\n",
      "MasVnrType       797\n",
      "MiscFeature     1328\n",
      "PoolQC          1372\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "na_vals = df.isna().sum()\n",
    "\n",
    "#filter out the columns that have 0 null values\n",
    "na_cols = na_vals[na_vals > 0]\n",
    "\n",
    "print(f\" == {len(na_cols)} Columns ==\")\n",
    "print(na_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen, the number of missing values for each column is very high. This indicates that the features entailed are not reliable for data analysis. The columns will be dropped to avoid losing the bulk of data from the dataset. The `DataFrame.dropna()` function is utilized along `axis = 1`, indicating the `columns` instead of rows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:59:02.713338500Z",
     "start_time": "2023-05-12T14:59:02.088803500Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Duplicates\n",
    "The dataset is checked for duplicated values using `DataFrame.duplicated()`. `sum()` yields the total number of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:59:02.726857800Z",
     "start_time": "2023-05-12T14:59:02.099411400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "No duplicates found. No further actions are needed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get numerical & categorical features\n",
    "The dataset is analyzed against the presence of categorical features. This is in order to later encode them to be ready for classification.\n",
    "`DataFrame.select_dtypes` returns the subset of the data of a specific datatype or superset of datatypes.\n",
    "A `pandas.Index` is created from the subtraction of the set of numerical features from the total set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return columns of `np.number` datatype\n",
    "num_cols = df.select_dtypes(np.number).columns\n",
    "\n",
    "#substract numerical columns from the total set of columns to get categorical features\n",
    "cat_cols = pd.Index(list(set(df.columns) - set(num_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `cat_cols` index is used to index `df` to obtain the number of unique values for each feature. This is accomplished using `DataFrame.nunique()`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for each feature\n",
    "df[cat_cols].nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical features\n",
    "The `LabelEncoder` class from scikit-learn is imported to assign a unique integer value for each unique category.\n",
    "The `LabelEncoder.fit_transform()` function firstly computes the unique integers for each feature and then transforms the original dataset `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#Iterate over the categorical features of df and transform them\n",
    "for cat_col in cat_cols:\n",
    "    df[cat_col] = le.fit_transform(df[cat_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be observed, the data is now purely numerical."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Removal\n",
    "In this section, the dataset will be analyzed against the presence of outliers, and if detected, will be removed.\n",
    "The target variable \"SalePrice\" is stored in variable `y`. It is, then, visualized using a Box Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"SalePrice\"]\n",
    "y.plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be observed, a number of outliers is are located outside the third quartile. Below, it is seen that these data points are only 76. They will be removed, for 76 is a small number compared to the size of the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of sample above certain price\n",
    "max_price = 325000\n",
    "print(len(y[y > max_price]))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "`df` is reassigned to the subset of data below the `max_price`. The dataset is then split into features `X` and target `y` i.e. the \"SalePrice\" feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples above that number are considered as outliers\n",
    "df = df[y < max_price]\n",
    "\n",
    "# Separate data into features and target\n",
    "x = df.drop(\"SalePrice\", axis=1)\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "The `StandardScaler` class is imported from scikit-learn to standardize features stored in `x` by removing the mean and scaling to unit variance. `fit_transform` is explained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale X\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "pd.DataFrame(x).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Grouping\n",
    "In order to classify housing prices into a specified set of classes, the target variable \"SalePrice\" needs to be divided into classes. This is accomplished using clustering techniques, as the classes are not defined a priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the box plot of the target variable `y`\n",
    "y.plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Show the KDE plot of the target variable `y`\n",
    "y.plot(kind = \"kde\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the plot above, it can be concluded that the `y` is not skewed, i.e. distributed somewhat equally around the mean."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The target `y` will be clustered into distinct groups using the scikit-learn class KMeans, which is an implementation of the k-means clustering algorithm.\n",
    "The number of clusters `k` is set to 4.\n",
    "`random_state` is used to produce repeatable results.\n",
    "`n_init` is the number of times the k-means algorithm is run with different centroid seeds. it is set to 'auto'.\n",
    "As required by the KMeans class, `y` is reshaped into a 2D array using `numpy.reshape()`.\n",
    "\n",
    "`KMeans.fit_predict()` returns an array of cluster values, each for each data point.\n",
    "`KMeans.cluster_centers_` stores the center of cluster for each data point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#initialize kmeans clustering algorithm\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=10, n_init='auto')\n",
    "\n",
    "#predict cluster for each data point, store them in y_clustered\n",
    "y_clustered = pd.Series(kmeans.fit_predict(np.array(y).reshape(-1, 1)))\n",
    "\n",
    "#store cluster center for each data point\n",
    "y_centers = pd.Series(kmeans.cluster_centers_[i] for i in y_clustered).astype(\"float64\")\n",
    "\n",
    "#view y_clustered\n",
    "y_clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "#view the centers and number of data points assigned to each\n",
    "y_centers.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`y` and `y_centers` are plotted using histograms to show the distribution of data before clustering and its distribution in the individual clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5), ncols=2)\n",
    "b = 10\n",
    "\n",
    "y.plot(color=\"#004D98\", kind=\"hist\", bins=b, ax=ax[0])\n",
    "ax[0].set_title('Distribution of Housing Prices')\n",
    "ax[0].set_xlabel('Sale Price')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "y_centers.plot(color=\"#DB0030\", kind=\"hist\", bins=b, ax=ax[1])\n",
    "ax[1].set_title('Distribution of Housing Prices')\n",
    "ax[1].set_xlabel('Sale Price')\n",
    "ax[1].set_ylabel('Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "After the preliminary analysis and exploration of the dataset, it is evident that the dataset contains a lot of unwanted or unhelpful features. These features can worsen the model or cause it to over-fit to the training data. To avoid this, the use of feature selection is needed.\n",
    "\n",
    "Two feature selection algorithms from scikit-learn will be used:\n",
    "- `SelectPercentile` which selects the top percentile of the features after applying the scoring function.\n",
    "- `SelectFromModel` which selects the features that best fit a certain Machine Learning model. Two models are used: `ExtraTreesClassifier` and `RandomForestClassifier`, both of which are classification algorithms that are based on Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following statement gets a `pandas.Index` of feature names, excluding the target variable \"SalePrice\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Index of feature names\n",
    "features = df.drop(\"SalePrice\", axis=1).columns\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection by percentile\n",
    "In the following cell, features will be selected based on a percentile of features ranked by their scores. This will utilize the `SelectPercentile` class. Scores will be determined using the `f_classif` function which implements the ANOVA(Analysis Of Variance) F-value method.\n",
    "\n",
    "The `f_sel` object of `SelectPercentile` returns `x_fcif` which is the subset of columns it has chosen via its `fit_transform` function. The features and target are inputs to the function. A percentile of 50% of the features will be chosen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature selection algorithms\n",
    "from sklearn.feature_selection import SelectFromModel, SelectPercentile\n",
    "# Import scoring function\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# f_classification\n",
    "f_sel = SelectPercentile(score_func=f_classif, percentile=50)\n",
    "x_fcif = f_sel.fit_transform(x, y)\n",
    "x_fcif.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection for an Extra Trees model\n",
    "In this algorithm, features will be chosen for an Extra Trees classification model. The model is firstly imported, instantiated as `etc`, initialized with 50 estimators(trees), and, then, fit to the data.\n",
    "\n",
    "The selector is instantiated as `etc_sel`, initialized with the model, and it is specified that the model has been fit before feature selection using `prefit = True`.\n",
    "The model is pre-fit to improve accuracy of feature selection.\n",
    "\n",
    "`x_etc` is, similarly, the subset of chosen columns, however, it is obtained from the `transform` function instead of `fit_transform`, because the model was pre-fit to the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Import ExtraTrees classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Initialize model with 50 estimators and fit to data\n",
    "etc = ExtraTreesClassifier(n_estimators=50).fit(x, y)\n",
    "\n",
    "# Select from model\n",
    "etc_sel = SelectFromModel(etc, prefit=True)\n",
    "x_etc = etc_sel.transform(x)\n",
    "x_etc.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection for a Random Forest model\n",
    "This cell is similar to the previous one, only different in the type of model, which, in this case, is the `RandomForestClassifier`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# Import RandomForest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize model with 50 estimators and fit to data\n",
    "rfc = RandomForestClassifier(n_estimators=50).fit(x, y)\n",
    "\n",
    "# Select from model\n",
    "rfc_sel = SelectFromModel(rfc, prefit=True)\n",
    "x_rfc = rfc_sel.transform(x)\n",
    "x_rfc.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Analyzing Selections\n",
    "After running three s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = pd.Series(np.concatenate((f_sel.get_feature_names_out(features),\n",
    "                                    etc_sel.get_feature_names_out(features),\n",
    "                                    rfc_sel.get_feature_names_out(features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "mask1 = f_sel.get_support()\n",
    "mask2 = etc_sel.get_support()\n",
    "mask3 = rfc_sel.get_support()\n",
    "\n",
    "mask  = np.logical_and(mask1, mask2, mask3)\n",
    "\n",
    "x_selected = x[:,mask]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train_evaluate() function\n",
    "\n",
    "This function trains and evaluates a machine learning model on a given dataset. The function takes the following arguments:\n",
    "\n",
    "`model`: The machine learning model to train and evaluate.\n",
    "`x`: The feature data.\n",
    "`y`: The target data.\n",
    "`grid_params`: A dictionary of hyperparameters to tune using grid search. If None, grid search will not be used.\n",
    "`cv`: The number of folds to use for cross-validation.\n",
    "\n",
    "The function first splits the data into train and test sets. If grid search is being used, the function then performs grid search to find the best hyperparameters for the model. The function then trains the model on the train set and evaluates the model on the test set. The function prints the classification report and confusion matrix for the model.\n",
    "\n",
    "The `train_evaluate()` function can be used to train and evaluate any machine learning model. It is a useful function for comparing different models and for finding the best hyperparameters for a given model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "def train_evaluate(model, x, y, grid_params=None, cv=5):\n",
    "    # split data into train & test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=10)\n",
    "\n",
    "    # check for applying grid search\n",
    "    if grid_params is None:\n",
    "        model.fit(x_train, y_train)\n",
    "    else:\n",
    "        model_gs = GridSearchCV(model, grid_params, scoring='balanced_accuracy', cv=cv, return_train_score=True)\n",
    "        model_gs.fit(x_train, y_train)\n",
    "        print(model_gs.best_params_)\n",
    "        model = model_gs.best_estimator_\n",
    "\n",
    "    # predict test results\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # show results\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion_Matrix:\")\n",
    "    ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNeighborsClassifier??\n",
    "grid_params = {\"n_neighbors\": [5, 7, 9], \"weights\": [\"uniform\", \"distance\"]}\n",
    "knn = train_evaluate(KNeighborsClassifier(), x_selected, y_clustered, grid_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = train_evaluate(GaussianNB(), x_selected, y_clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "mnb = train_evaluate(BernoulliNB(), x_selected, y_clustered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "s_knn = KNeighborsClassifier(n_neighbors=9, weights=\"distance\")\n",
    "s_gnb = GaussianNB()\n",
    "s_bnb = BernoulliNB()\n",
    "stacked = StackingClassifier(estimators=[('knn', s_knn), ('gnb', s_gnb)], final_estimator=s_knn)\n",
    "\n",
    "train_evaluate(stacked, x_selected, y_clustered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
